# Benchmark Configuration
# Config-driven benchmarking framework for ML inference implementations

[benchmark]
name = "DistilBERT Sentiment Analysis Benchmark"
description = "Performance comparison: MAX Engine vs HuggingFace PyTorch"
warmup_iterations = 100
test_iterations = 1000
save_results = true
results_dir = "results"
timestamp_results = true  # Add timestamp to result filenames

[model]
path = "../../models/distilbert-sentiment"
task = "sentiment-analysis"

[test_data]
# Path to benchmark data (JSONL format)
benchmark_file = "benchmark_data/sentiment_benchmark.jsonl"  # Relative to this file

# Path to validation data for correctness checking
validation_file = "benchmark_data/sentiment_validation.jsonl"  # Relative to this file

# Number of times to repeat the benchmark dataset
repeat = 20  # 50 samples Ã— 20 = 1000 samples for benchmark iterations

# Optional: filter by category (empty = use all)
# categories = ["short", "medium"]  # Uncomment to filter

[implementations.max]
enabled = true
name = "MAX Engine"
description = "Custom MAX Graph implementation with DistilBERT"
type = "custom"
module = "src.python.max_distilbert.inference"
class_name = "DistilBertSentimentClassifier"
# Custom initialization args (passed to __init__)
init_args = { model_path = "{{model.path}}" }

[implementations.huggingface_cpu]
enabled = true
name = "HuggingFace (PyTorch CPU)"
description = "Transformers pipeline on CPU"
type = "huggingface_pipeline"
device = "cpu"

[implementations.huggingface_mps]
enabled = false  # Enable for Apple Silicon GPU testing
name = "HuggingFace (PyTorch MPS)"
description = "Transformers pipeline on Apple Silicon GPU"
type = "huggingface_pipeline"
device = "mps"

[implementations.onnx]
enabled = false  # Enable for ONNX Runtime comparison
name = "ONNX Runtime"
description = "ONNX optimized inference"
type = "onnx"
model_path = "models/distilbert-sentiment/model.onnx"

[output]
# Reporting precision (significant figures)
latency_sigfigs = 3      # e.g., 23.6 ms, 1.45 ms
throughput_sigfigs = 3   # e.g., 42.4 req/s, 965 req/s
ratio_sigfigs = 3        # e.g., 1.45x speedup, CV 0.341
percentage_decimals = 1  # e.g., 44.6%
confidence_decimals = 4  # e.g., 0.9999

# Metrics to calculate and report
metrics = [
    "mean",
    "median",
    "p50",
    "p95",
    "p99",
    "min",
    "max",
    "std",
    "throughput"
]

# Output formats (multiple allowed)
formats = ["console", "json", "csv", "markdown"]

# Include per-iteration raw data in output
include_raw_data = false

# Generate visualization plots (requires matplotlib)
generate_plots = false

# Comparison baseline (name of implementation to compare against)
baseline = "huggingface_cpu"

[output.console]
# Console output styling
style = "table"  # Options: table, simple, detailed
show_comparison = true
show_speedup = true
highlight_best = true

[output.json]
pretty_print = true
indent = 2

[output.csv]
delimiter = ","
include_header = true
